{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd177216",
      "metadata": {
        "id": "fd177216"
      },
      "source": [
        "# Clasificación de Noticias Falsas con TF-IDF + Random Forest\n",
        "\n",
        "Este notebook mejora el preprocesamiento para textos en español con eliminación de stopwords y tokenización eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bb6b4f",
      "metadata": {
        "id": "16bb6b4f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carga de datos\n",
        "ruta = \"/content/Dataset_Completo.xlsx\"\n",
        "df = pd.read_excel(ruta)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4756e6c8",
      "metadata": {
        "id": "4756e6c8"
      },
      "source": [
        "## Preprocesamiento con limpieza, tokenización y eliminación de stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389008df",
      "metadata": {
        "id": "389008df"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stopwords español\n",
        "stopwords_es = set(stopwords.words('spanish'))\n",
        "\n",
        "def limpiar_y_tokenizar(texto):\n",
        "    texto = str(texto)\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"http\\S+\", \"\", texto)\n",
        "    texto = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", texto)\n",
        "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
        "    tokens = texto.split()\n",
        "    tokens = [t for t in tokens if t not in stopwords_es and len(t) > 2]\n",
        "    tokens = [t for t in tokens if not t.isnumeric()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Combinar y limpiar. Inluimos en el campo texto_limpio la combinación de Topic, Source, Headline y Text\n",
        "df[\"Headline\"] = df[\"Headline\"].fillna(\"\")\n",
        "df[\"Text\"] = df[\"Text\"].fillna(\"\")\n",
        "df[\"texto_limpio\"] = (\n",
        "    \"Topic: \" + df[\"Topic\"] + \". \" +\n",
        "    \"Source: \" + df[\"Source\"] + \". \" +\n",
        "    \"Headline: \" + df[\"Headline\"] + \" \" +\n",
        "    df[\"Text\"]\n",
        ").apply(limpiar_y_tokenizar)\n",
        "\n",
        "df[\"Category\"] = df[\"Category\"].astype(str).str.strip().str.lower().map({\"fake\": 1, \"true\": 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eca8176",
      "metadata": {
        "id": "0eca8176"
      },
      "source": [
        "## División del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d614c7",
      "metadata": {
        "id": "95d614c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[\"texto_limpio\"]\n",
        "y = df[\"Category\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e61cd3e",
      "metadata": {
        "id": "5e61cd3e"
      },
      "source": [
        "## Vectorización con TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b36222d",
      "metadata": {
        "id": "2b36222d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=list(stopwords_es)\n",
        ")\n",
        "\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f27ad6",
      "metadata": {
        "id": "08f27ad6"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822262cb",
      "metadata": {
        "id": "822262cb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train_vec, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee9477a3",
      "metadata": {
        "id": "ee9477a3"
      },
      "source": [
        "## Evaluación: Métricas, Matriz de Confusión y Curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb84cba",
      "metadata": {
        "id": "ecb84cba"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Predicciones y probabilidades\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "y_pred_num = pd.Series(y_pred).map({0: 0, 1: 1})\n",
        "y_proba = clf.predict_proba(X_test_vec)[:, 1]\n",
        "\n",
        "# Métricas\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_num))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_num))\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, y_pred_num, target_names=[\"Fake\", \"Real\"]))\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred_num)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Matriz de Confusión - TF-IDF\")\n",
        "plt.show()\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"Curva ROC (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"Curva ROC - Random Forest con TF-IDF\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prueba con nuevas noticias"
      ],
      "metadata": {
        "id": "XpA5MngmK9N3"
      },
      "id": "XpA5MngmK9N3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevas noticias a clasificar\n",
        "nuevas_noticias = [\n",
        "    \"El pasado mes cuatro soldados resultaron heridos en Croacia cerca de un instituto nacional\",\n",
        "    \"El presidente esta mañana en el Congreso anunció nuevas medidas ante la llegada de aliens.\"\n",
        "]\n",
        "\n",
        "# Usar la función mejorada de limpieza y tokenización\n",
        "nuevas_limpias = [limpiar_y_tokenizar(n) for n in nuevas_noticias]\n",
        "nuevas_vec = vectorizer.transform(nuevas_limpias)\n",
        "predicciones = clf.predict(nuevas_vec)\n",
        "\n",
        "# Mostrar resultados\n",
        "for noticia, pred in zip(nuevas_noticias, predicciones):\n",
        "    etiqueta = \"Fake\" if pred == 0 else \"Real\"\n",
        "    print(f\" {noticia}\\n Predicción: {etiqueta}\\n\")\n"
      ],
      "metadata": {
        "id": "yUL4sgq3FlnB"
      },
      "id": "yUL4sgq3FlnB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}