{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEMorkUQqRFK"
      },
      "source": [
        "# Carga y análisis del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v31cGuq4LTyf"
      },
      "source": [
        "Instalar librerías necesarias (si no están instaladas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtyEB6awnhMg"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl matplotlib seaborn --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install numpy==1.26.4 --quiet # (versión anterior)\n",
        "\n",
        "# Después de ejecutar estos comandos es necesario reiniciar el kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy-2w2cNLbAe"
      },
      "source": [
        "Importar librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuw1WylvnkVr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpqlA2f2Liz3"
      },
      "source": [
        "Carga del dataset y normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Nmc6yPnoQr"
      },
      "outputs": [],
      "source": [
        "ruta = \"/content/Dataset_Completo.xlsx\"\n",
        "df = pd.read_excel(ruta)\n",
        "\n",
        "# Limpieza y normalización de la columna 'Category'\n",
        "df['Category'] = df['Category'].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Mapeo de etiquetas: 'true' → 0 y 'fake' → 1\n",
        "df['label'] = df['Category'].map({'true': 0, 'fake': 1})\n",
        "\n",
        "# Verificación\n",
        "print(\"Valores únicos en 'Category':\", df['Category'].unique())\n",
        "print(\"Valores únicos en 'label':\", df['label'].unique())\n",
        "print(\"\\nDistribución de clases:\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCo-2vGeL9sv"
      },
      "source": [
        "# Entrenamiento de BERT con el texto completo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Usamos 'text', 'topic', 'source' y 'headline' como entrada. El campo LINK no lo utilizamos para el entrenamiento\n",
        "df_texto = df[['Text', 'Topic', 'Source', 'Headline', 'label']].dropna().copy()\n",
        "\n",
        "# Renombramos todas las columnas\n",
        "df_texto.rename(columns={\n",
        "    'Text': 'text',\n",
        "    'Topic': 'topic',\n",
        "    'Source': 'source',\n",
        "    'Headline': 'headline'\n",
        "}, inplace=True)\n",
        "\n",
        "# 2. División del dataset en entrenamiento y prueba\n",
        "train_df, test_df = train_test_split(df_texto, test_size=0.2, stratify=df_texto['label'], random_state=42)\n",
        "\n",
        "# 3. Conversión a datasets de Hugging Face\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# 4. Tokenizador y tokenización\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 5. Formateo\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# 6. Modelo multilingüe\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# 7. Configuración de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_multilingual\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    eval_strategy=\"epoch\",  # Para evaluar cada época\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 8 Preparación metricas entrenamiento\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# 9. Entrenador\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Espera 2 épocas sin mejorar\n",
        ")\n",
        "\n",
        "# 9. Entrenar el modelo\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "G8cal9IfgDsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo entrenado"
      ],
      "metadata": {
        "id": "QzWlJm0woTB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar modelo y tokenizer tras el entrenamiento\n",
        "trainer.save_model(\"./modelo_bert_multilingual\")\n",
        "tokenizer.save_pretrained(\"./modelo_bert_multilingual\")"
      ],
      "metadata": {
        "id": "vAwvbXAqoOjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación del modelo con texto completo"
      ],
      "metadata": {
        "id": "CR7-ETCKgdaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# 1. Obtener predicciones\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# 2. Métricas principales\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision = precision_score(labels, preds)\n",
        "recall = recall_score(labels, preds)\n",
        "f1 = f1_score(labels, preds)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# 3. Reporte completo por clase\n",
        "print(\"\\nReporte completo:\")\n",
        "print(classification_report(labels, preds, target_names=[\"True\", \"Fake\"]))\n",
        "\n",
        "# 4. Matriz de confusión\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"True\", \"Fake\"], yticklabels=[\"True\", \"Fake\"])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusión - Texto completo (BERT)')\n",
        "plt.show()\n",
        "\n",
        "# 5. Curva ROC y AUC\n",
        "probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n",
        "auc = roc_auc_score(labels, probs)\n",
        "fpr, tpr, _ = roc_curve(labels, probs)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title('Curva ROC - Modelo con texto completo (BERT)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OouagLS1gDoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}